{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca1c3a5-f483-4220-b49f-817c38f7985c",
   "metadata": {},
   "source": [
    "## Simple pipeline with MLflow model tracking\n",
    "\n",
    "An experiment with iris dataset. In general, to use MLflow in a Kubeflow Pipeline, the necessary environment should be passed to the containers using the MLflow logic. This is implemented in the `add_env_to_mlflow_kfp_container` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a8547-37f3-4f4c-aad3-a9f9fd7f3fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "from kfp.client import Client\n",
    "from kfp.dsl import Dataset, Input, Model, Output\n",
    "\n",
    "\n",
    "def add_env_to_mlflow_kfp_container(container_task):\n",
    "    # these variables are needed by MLflow\n",
    "    mlflow_vars = {\n",
    "        \"AWS_ENDPOINT_URL\": os.environ[\"AWS_ENDPOINT_URL\"],\n",
    "        \"AWS_ACCESS_KEY_ID\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        \"AWS_SECRET_ACCESS_KEY\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        \"DB_URI\": os.environ[\"DB_URI\"],\n",
    "        \"MLFLOW_S3_BUCKET\": os.environ[\"MLFLOW_S3_BUCKET\"],\n",
    "        \"MLFLOW_URI\": os.environ[\"MLFLOW_URI\"]\n",
    "    }\n",
    "    for key, value in mlflow_vars.items():\n",
    "        container_task.set_env_variable(key, value)\n",
    "    return container_task\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.11\",\n",
    ")\n",
    "def preprocess_data(\n",
    "    x_train_df: Output[Dataset],\n",
    "    y_train_df: Output[Dataset],\n",
    "    x_test_df: Output[Dataset],\n",
    "    y_test_df: Output[Dataset],\n",
    "    test_size: float = 0.2,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Reads iris data and writes it to pipeline artifacts as parquet.\"\"\"\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = datasets.load_iris(as_frame=True)\n",
    "    x = df.data\n",
    "    y = df.target.to_frame()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "\n",
    "    for obj, artifact in zip(\n",
    "        (x_train, x_test, y_train, y_test),\n",
    "        (x_train_df, x_test_df, y_train_df, y_test_df)\n",
    "    ):\n",
    "        obj.to_parquet(artifact.path)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n",
    "    base_image=\"python:3.11\",\n",
    ")\n",
    "def train_and_log_model(\n",
    "    x_train: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    x_test: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    trained_model: Output[Model],\n",
    "    seed: int = 42,\n",
    "):\n",
    "    import os\n",
    "\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    from joblib import dump\n",
    "    from mlflow.models import infer_signature\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    x_train = pd.read_parquet(x_train.path)\n",
    "    y_train = pd.read_parquet(y_train.path)\n",
    "    x_test = pd.read_parquet(x_test.path)\n",
    "    y_test = pd.read_parquet(y_test.path)\n",
    "\n",
    "    # Define the model hyperparameters\n",
    "    params = {\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 1000,\n",
    "        \"multi_class\": \"auto\",\n",
    "        \"random_state\": seed,\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    lr = LogisticRegression(**params)\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    dump(lr, trained_model.path)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = lr.predict(x_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Set our tracking server uri for logging\n",
    "    mlflow.set_tracking_uri(uri=os.environ[\"MLFLOW_URI\"])\n",
    "\n",
    "    # Create a new MLflow Experiment\n",
    "    mlflow.set_experiment(\"MLflow Quickstart with KFP\")\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log the loss metric\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Set a tag that we can use to remind ourselves what this run was for\n",
    "        mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data, KFP\")\n",
    "\n",
    "        # Infer the model signature\n",
    "        signature = infer_signature(x_train, lr.predict(x_train))\n",
    "\n",
    "        # Log the model\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=lr,\n",
    "            artifact_path=\"iris_model\",\n",
    "            signature=signature,\n",
    "            input_example=x_train,\n",
    "            registered_model_name=\"tracking-quickstart-pipeline\",\n",
    "        )\n",
    "\n",
    "\n",
    "@dsl.pipeline\n",
    "def simple_pipeline():\n",
    "\n",
    "    # Step 1: Preprocess the data\n",
    "    preprocess_data_task = preprocess_data()\n",
    "\n",
    "    # Step 2: Train the model and add necessary env vars\n",
    "    train_and_log_model_task = train_and_log_model(\n",
    "        x_train=preprocess_data_task.outputs['x_train_df'],\n",
    "        y_train=preprocess_data_task.outputs['y_train_df'],\n",
    "        x_test=preprocess_data_task.outputs['x_test_df'],\n",
    "        y_test=preprocess_data_task.outputs['y_test_df'],\n",
    "    )\n",
    "    train_and_log_model_task = add_env_to_mlflow_kfp_container(train_and_log_model_task)\n",
    "\n",
    "\n",
    "# Initialize the Kubeflow Pipelines client\n",
    "client = Client()\n",
    "\n",
    "# Create a new run from the pipeline function\n",
    "client.create_run_from_pipeline_func(\n",
    "    simple_pipeline,\n",
    "    experiment_name=\"iris-dataset-classification\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "# kfp.compiler.Compiler().compile(simple_pipeline, 'simple_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
