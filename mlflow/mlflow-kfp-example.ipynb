{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Simple pipeline with MLflow model tracking\n",
    "\n",
    "An experiment with iris dataset. In general, to use MLflow in a Kubeflow Pipeline, the necessary environment variables should be passed to the containers using the MLflow logic. This is implemented in the `add_env_vars_to_tasks` function. \n",
    "  \n",
    "MLflow also needs to know the local URI for MLflow server, which is available in the notebook (and in containers through the mentioned function) using `os.environ[\"MLFLOW_URI\"]`.\n",
    "\n",
    "This notebook defines a simple pipeline for preprocessing data, training and logging model, and prediction on test data. It also shows one way to handle MLflow experiment info inside the notebook and passing it between experiments - by saving a run dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q kfp[all]==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "from kfp.client import Client\n",
    "from kfp.dsl import Dataset, Input, Model, Output, Artifact\n",
    "from kfp.kubernetes import use_secret_as_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_env_vars_to_tasks(task_list: list[dsl.PipelineTask]) -> None:\n",
    "    \"\"\"Adds environment variables for MinIO to the MLflow tasks\"\"\"\n",
    "    for task in task_list:\n",
    "        task.set_env_variable(\"MLFLOW_URI\", os.environ[\"MLFLOW_URI\"])\n",
    "        task.set_env_variable(\"AWS_ENDPOINT_URL\", os.environ[\"AWS_ENDPOINT_URL\"])\n",
    "        use_secret_as_env(\n",
    "            task,\n",
    "            secret_name=\"s3creds\",\n",
    "            secret_key_to_env={\n",
    "                \"AWS_ACCESS_KEY_ID\": \"AWS_ACCESS_KEY_ID\",\n",
    "                \"AWS_SECRET_ACCESS_KEY\": \"AWS_SECRET_ACCESS_KEY\",\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.11\",\n",
    ")\n",
    "def preprocess_data(\n",
    "    x_train_df: Output[Dataset],\n",
    "    y_train_df: Output[Dataset],\n",
    "    x_test_df: Output[Dataset],\n",
    "    y_test_df: Output[Dataset],\n",
    "    test_size: float = 0.2,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Reads iris data and writes it to pipeline artifacts as parquet.\"\"\"\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = datasets.load_iris(as_frame=True)\n",
    "    x = df.data\n",
    "    y = df.target.to_frame()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "\n",
    "    for obj, artifact in zip(\n",
    "        (x_train, x_test, y_train, y_test),\n",
    "        (x_train_df, x_test_df, y_train_df, y_test_df)\n",
    "    ):\n",
    "        obj.to_parquet(artifact.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Train and log model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "As an example of how to use MLflow with pipelines, this notebook saves MLflow run parameters as a dict. This dict can be loaded from other KFP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n",
    "    base_image=\"python:3.11\",\n",
    ")\n",
    "def train_and_log_model(\n",
    "    x_train: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    seed: int = 42,\n",
    ") -> dict:\n",
    "    \n",
    "    import os\n",
    "\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    from mlflow.models import infer_signature\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "    x_train = pd.read_parquet(x_train.path)\n",
    "    y_train = pd.read_parquet(y_train.path)\n",
    "\n",
    "    # Define the model hyperparameters\n",
    "    params = {\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 1000,\n",
    "        \"multi_class\": \"auto\",\n",
    "        \"random_state\": seed,\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    lr = LogisticRegression(**params)\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    # Set tracking server URI for logging\n",
    "    mlflow.set_tracking_uri(uri=os.environ[\"MLFLOW_URI\"])\n",
    "\n",
    "    # Create MLflow Experiment name\n",
    "    mlflow.set_experiment(\"MLflow Quickstart with KFP\")\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Set a tag that we can use to remind ourselves what this run was for\n",
    "        mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data, KFP\")\n",
    "\n",
    "        # Infer the model signature\n",
    "        signature = infer_signature(x_train, lr.predict(x_train))\n",
    "\n",
    "        # Log the model\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=lr,\n",
    "            artifact_path=\"iris-model\",\n",
    "            signature=signature,\n",
    "            input_example=x_train,\n",
    "            registered_model_name=\"tracking-quickstart-pipeline\",\n",
    "        )\n",
    "    \n",
    "    # Save run as dict\n",
    "    return run.to_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Load the model from MLflow and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "This component loads model saved to MLflow based on the run ID. Requires the dictionary with MLflow run information as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n",
    "    base_image=\"python:3.11\",\n",
    ")\n",
    "def predict(\n",
    "    x_test: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    mlflow_run: dict,\n",
    "):\n",
    "    import os\n",
    "\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    mlflow.set_tracking_uri(uri=os.environ[\"MLFLOW_URI\"])\n",
    "    \n",
    "    # Load trained model\n",
    "    run_id = mlflow_run[\"info\"][\"run_id\"]\n",
    "    model_path = f\"runs:/{run_id}/iris-model\"  # model name (iris-model) corresponds to artifact path \n",
    "    model = mlflow.sklearn.load_model(model_path)\n",
    "\n",
    "    # Load test data\n",
    "    x_test = pd.read_parquet(x_test.path)\n",
    "    y_test = pd.read_parquet(y_test.path)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate metric\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        # Log the loss metric\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Build and run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def simple_pipeline():\n",
    "\n",
    "    # Step 1: Preprocess the data\n",
    "    preprocess_data_task = preprocess_data()\n",
    "\n",
    "    # Step 2: Train the model and add necessary env vars\n",
    "    train_and_log_model_task = train_and_log_model(\n",
    "        x_train=preprocess_data_task.outputs['x_train_df'],\n",
    "        y_train=preprocess_data_task.outputs['y_train_df'],\n",
    "    )\n",
    "\n",
    "    # Step 3: Predict on test data\n",
    "    predict_task = predict(\n",
    "        x_test=preprocess_data_task.outputs['x_test_df'],\n",
    "        y_test=preprocess_data_task.outputs['y_test_df'],\n",
    "        mlflow_run=train_and_log_model_task.output,\n",
    "    )\n",
    "    \n",
    "    # Add env vars\n",
    "    add_env_vars_to_tasks([train_and_log_model_task, predict_task])\n",
    "\n",
    "\n",
    "# Initialize the Kubeflow Pipelines client\n",
    "client = Client()\n",
    "\n",
    "# Create a new run from the pipeline function\n",
    "client.create_run_from_pipeline_func(\n",
    "    simple_pipeline,\n",
    "    experiment_name=\"iris-dataset-classification\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "# kfp.compiler.Compiler().compile(simple_pipeline, 'simple_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
