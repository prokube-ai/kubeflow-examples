{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2b9238-eba2-4166-bfb5-bfba6e686a90",
   "metadata": {},
   "source": [
    "# Mobile Price Predictions\n",
    "\n",
    "In this notebook, you will learn how to build a Kubeflow pipeline using the lightweight components. This is the simplest way to get started with Kubeflow Pipelines.\n",
    "For this tutorial, we will utilize the Mobile Price Classification dataset available [on Kaggle](https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification?datasetId=11167&sortBy=voteCount).\n",
    "\n",
    "As a prerequisite, please download the dataset, unzip it, and upload it to MinIO. In our example, we uploaded both tables to a MinIO bucket, such that the respective paths are: 'kubeflow-examples/mobile-price-classification/test.csv' and 'kubeflow-examples/mobile-price-classification/train.csv'. These paths are input parameters for the pipeline. If you decide to store the files in a different location, you can easily modify these parameters in the \"Compile and run pipeline\" section below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e59d2-daa3-4968-9b59-f00ac22dca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp[all]==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9375be0-0baa-4540-8357-d7833f07dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import HTML, Input, Output, Dataset, Artifact, Model, ClassificationMetrics, Markdown\n",
    "from kfp.client import Client\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71469b33-4406-40e0-948f-988268c375b6",
   "metadata": {},
   "source": [
    "## Create components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6adfb40-28ff-4ebd-8bd5-e0ab5556662d",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73fce9-ef1e-4812-8a08-9bbe19bc6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def read_data(\n",
    "    minio_train_data: Input[Dataset],\n",
    "    minio_test_data: Input[Dataset],\n",
    "    train_df: Output[Dataset],\n",
    "    test_df: Output[Dataset],    \n",
    "):\n",
    "    \"\"\"Reads training and test data writes it to pipeline artifacts as parquet.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df_train = pd.read_csv(minio_train_data.path)\n",
    "    df_test = pd.read_csv(minio_test_data.path)\n",
    "    \n",
    "\n",
    "    df_train.to_parquet(train_df.path)\n",
    "    df_test.to_parquet(test_df.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c6a87-c491-4d9d-aad2-ae12bdb465b3",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc0e2c-395f-4e32-856d-1633d6e53caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def split_data(\n",
    "    train_df: Input[Dataset],\n",
    "    x_train_df: Output[Dataset],\n",
    "    y_train_df: Output[Dataset],\n",
    "    x_val_df: Output[Dataset],\n",
    "    y_val_df: Output[Dataset],\n",
    "    test_size: float = 0.5,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Splits the provided dataset into training and validation sets.\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split \n",
    "\n",
    "    # Read the input dataset\n",
    "    data = pd.read_parquet(train_df.path)\n",
    "\n",
    "    # Separate target from features\n",
    "    y = data[\"price_range\"].to_frame()\n",
    "    x_data = data.drop([\"price_range\"], axis=1)\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # Save the splitted data to their respective output paths\n",
    "    for object, artifact in zip((x_train, x_val, y_train, y_val), (x_train_df, x_val_df, y_train_df, y_val_df)):\n",
    "        object.to_parquet(artifact.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17300970-4b71-47a4-9eb8-f648571cbbcd",
   "metadata": {},
   "source": [
    "### Fit scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b8842-7466-4338-a559-4801fea6f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def fit_scaler(\n",
    "    train_x: Input[Dataset],\n",
    "    fitted_scaler: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits a MinMaxScaler on the provided training data and saves the fitted scaler.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read the input dataset\n",
    "    x_train = pd.read_parquet(train_x.path)\n",
    "    \n",
    "    # Fit the MinMaxScaler on the training data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    \n",
    "    # Save the fitted scaler\n",
    "    dump(scaler, fitted_scaler.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d5fca-398d-450d-b1b3-166088f8dc00",
   "metadata": {},
   "source": [
    "### Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c239b-401c-4b7a-ab67-d98e2464a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def tune_hyperparams(\n",
    "    train_x: Input[Dataset],\n",
    "    train_y: Input[Dataset],\n",
    "    fitted_scaler: Input[Artifact],\n",
    "    C: List = [1, 0.1, 0.25, 0.5, 2, 0.75],\n",
    "    kernel: List = [\"linear\", \"rbf\"],\n",
    "    gamma: List = [\"auto\", 0.01, 0.001, 0.0001, 1],\n",
    "    decision_function_shape: List[str] = [\"ovo\", \"ovr\"],\n",
    "    seed: int = 42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using GridSearchCV for a SVM classifier on the provided training data.\n",
    "    Returns the best hyperparameters found.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    from joblib import load\n",
    "\n",
    "    # Load the fitted scaler\n",
    "    scaler = load(fitted_scaler.path)\n",
    "\n",
    "    # Read and preprocess the training data\n",
    "    x_train, y_train = [pd.read_parquet(path) for path in (train_x.path, train_y.path)]\n",
    "    x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
    "\n",
    "    # Initialize SVM with a random seed\n",
    "    svm = SVC(random_state=seed)\n",
    "\n",
    "    # Define grid search with provided hyperparameters\n",
    "    grid_svm = GridSearchCV(\n",
    "        estimator=svm,\n",
    "        cv=5,\n",
    "        param_grid=dict(\n",
    "            kernel=kernel, \n",
    "            C=C, \n",
    "            gamma=gamma, \n",
    "            decision_function_shape=decision_function_shape\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_svm.fit(x_train, y_train['price_range'].values)\n",
    "    \n",
    "    # Print the best score found\n",
    "    print(\"Best score: \", grid_svm.best_score_)\n",
    "\n",
    "    # Return the best hyperparameters\n",
    "    return grid_svm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6ede5-c3e0-4d6d-bb30-860f3c122359",
   "metadata": {},
   "source": [
    "### Train model with optimal hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c579baf-1164-4652-a864-45dc9c48de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def train_model(\n",
    "    train_x: Input[Dataset],\n",
    "    train_y: Input[Dataset],\n",
    "    fitted_scaler: Input[Artifact],\n",
    "    hparams: Dict,\n",
    "    trained_model: Output[Model],\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains an SVM classifier on the provided training data using the best hyperparameters from tuning.\n",
    "    The trained model is then saved.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.svm import SVC\n",
    "    from joblib import dump, load\n",
    "\n",
    "    # Load the fitted scaler\n",
    "    scaler = load(fitted_scaler.path)\n",
    "\n",
    "    # Read and preprocess the training data\n",
    "    x_train, y_train = [pd.read_parquet(path) for path in (train_x.path, train_y.path)]\n",
    "    x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
    "\n",
    "    # Initialize SVM with the best hyperparameters and a random seed\n",
    "    svm_model = SVC(random_state=seed, **hparams)\n",
    "\n",
    "    # Train the SVM model\n",
    "    svm_model.fit(x_train, y_train['price_range'].values)\n",
    "\n",
    "    # Save the trained model\n",
    "    dump(svm_model, trained_model.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9ef30-3352-44ec-a0e4-9e52f032194e",
   "metadata": {},
   "source": [
    "### Evaluate model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9453f2-0c56-4c25-af05-7dfe42eaf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def evaluate_model(\n",
    "    val_x: Input[Dataset],\n",
    "    val_y: Input[Dataset],\n",
    "    fitted_scaler: Input[Artifact],\n",
    "    trained_model: Input[Model],\n",
    "    confusion_matrix_plot: Output[ClassificationMetrics],\n",
    "    classification_report_md: Output[Markdown]\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained SVM model using validation data.\n",
    "    Outputs a confusion matrix plot and a markdown file containing a classification report.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.svm import SVC\n",
    "    from joblib import load\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "    # Load the fitted scaler and preprocess validation data\n",
    "    scaler = load(fitted_scaler.path)\n",
    "    x_val, y_val = [pd.read_parquet(path) for path in (val_x.path, val_y.path)]\n",
    "    x_val = pd.DataFrame(scaler.transform(x_val), columns=x_val.columns)\n",
    "\n",
    "    # Load the trained SVM model and make predictions on validation data\n",
    "    svm_model = load(trained_model.path)\n",
    "    predictions = svm_model.predict(x_val)\n",
    "\n",
    "    # Log the confusion matrix\n",
    "    confusion_matrix_plot.log_confusion_matrix(\n",
    "        [str(v) for v in y_val['price_range'].unique()],\n",
    "        confusion_matrix(y_val['price_range'].values.tolist(), predictions.tolist()).tolist()\n",
    "    )\n",
    "\n",
    "    # Create the markdown content for classification report\n",
    "    markdown_content = f\"```\\n{classification_report(y_val['price_range'].values, predictions)}\\n```\"\n",
    "\n",
    "    # Write the content to a Markdown file\n",
    "    with open(classification_report_md.path, 'w') as f:\n",
    "        f.write(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836412b8-fe89-48e9-8383-53a5db9761bf",
   "metadata": {},
   "source": [
    "### Run predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e40c2d-889e-4cc6-bb9d-a722653acb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"pyarrow\", \"plotly\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def test_model(\n",
    "    test_x: Input[Dataset],\n",
    "    trained_model: Input[Model],\n",
    "    fitted_scaler: Input[Artifact],\n",
    "    column_x: str,\n",
    "    column_y: str,\n",
    "    scatter_plot: Output[HTML]\n",
    "):\n",
    "    \"\"\"\n",
    "    Test a trained SVM model on provided test data and produce a scatter plot.\n",
    "    The scatter plot will have points colored by the predicted class based on two columns\n",
    "    specified by the user.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from joblib import load\n",
    "    import plotly.express as px\n",
    "\n",
    "    # Load the fitted scaler and preprocess test data\n",
    "    scaler = load(fitted_scaler.path)\n",
    "    x_test = pd.read_parquet(test_x.path)\n",
    "    x_test = x_test.drop('id', axis=1)\n",
    "    x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "\n",
    "    # Load the trained SVM model and make predictions on test data\n",
    "    svm_model = load(trained_model.path)\n",
    "    predictions = svm_model.predict(x_test)\n",
    "\n",
    "    # Add predictions as a column to the x_test DataFrame for visualization\n",
    "    x_test['Predicted Class'] = predictions\n",
    "\n",
    "    # Create the scatter plot using plotly\n",
    "    fig = px.scatter(x_test,\n",
    "                     x=column_x,\n",
    "                     y=column_y,\n",
    "                     color='Predicted Class',\n",
    "                     color_continuous_scale='Viridis',\n",
    "                     title=f\"Scatter plot of {column_x} vs. {column_y} colored by Predicted Class\",\n",
    "                     template='plotly_dark')\n",
    "\n",
    "    # Save the plot as an HTML file\n",
    "    fig.write_html(scatter_plot.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fab6f-5dc8-47e0-b15f-868032f076b2",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f3431-4429-4474-8c66-e57a1ce80d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def mobile_price_classification_pipeline(\n",
    "    minio_train_data_path: str = 'minio://kubeflow-examples/mobile-price-classification/train.csv',\n",
    "    minio_test_data_path: str = 'minio://kubeflow-examples/mobile-price-classification/test.csv',\n",
    "    test_size: float = 0.5,\n",
    "    C: List = [1, 0.1, 0.25, 0.5, 2, 0.75],\n",
    "    kernel: List = [\"linear\", \"rbf\"],\n",
    "    gamma: List = [\"auto\", 0.01, 0.001, 0.0001, 1],\n",
    "    decision_function_shape: List[str] = [\"ovo\", \"ovr\"],\n",
    "    scatter_plot_column_x: str = 'ram',\n",
    "    scatter_plot_column_y: str = 'battery_power',\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Define the mobile price classification pipeline.\n",
    "    \n",
    "    This pipeline covers the following steps:\n",
    "    1. Read data from the specified paths.\n",
    "    2. Split the data into training and validation sets.\n",
    "    3. Fit the MinMax scaler.\n",
    "    4. Tune hyperparameters for the SVM model.\n",
    "    5. Train the SVM model with the best hyperparameters.\n",
    "    6. Evaluate the trained model.\n",
    "    7. Test the model and visualize the results with a scatter plot.\n",
    "    \"\"\"\n",
    "    import_train = dsl.importer(\n",
    "        artifact_uri=minio_train_data_path,\n",
    "        artifact_class=dsl.Dataset,\n",
    "        reimport=True)\n",
    "    \n",
    "    import_test = dsl.importer(\n",
    "        artifact_uri=minio_test_data_path,\n",
    "        artifact_class=dsl.Dataset,\n",
    "        reimport=True)\n",
    "    \n",
    "    # Step 1: Read the data\n",
    "    read_data_task = read_data(\n",
    "        minio_train_data=import_train.output,\n",
    "        minio_test_data=import_test.output\n",
    "    )\n",
    "\n",
    "    # Step 2: Split the data\n",
    "    split_data_task = split_data(\n",
    "        train_df=read_data_task.outputs['train_df'],\n",
    "        test_size=test_size,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Step 3: Fit the scaler\n",
    "    fit_scaler_task = fit_scaler(\n",
    "        train_x=split_data_task.outputs['x_train_df']\n",
    "    )\n",
    "\n",
    "    # Step 4: Tune hyperparameters\n",
    "    tune_hyperparams_task = tune_hyperparams(\n",
    "        train_x=split_data_task.outputs['x_train_df'],\n",
    "        train_y=split_data_task.outputs['y_train_df'],\n",
    "        fitted_scaler=fit_scaler_task.outputs['fitted_scaler']\n",
    "    )\n",
    "\n",
    "    # Step 5: Train the model\n",
    "    train_model_task = train_model(\n",
    "        train_x=split_data_task.outputs['x_train_df'],\n",
    "        train_y=split_data_task.outputs['y_train_df'],\n",
    "        hparams=tune_hyperparams_task.output,\n",
    "        fitted_scaler=fit_scaler_task.outputs['fitted_scaler']\n",
    "    )\n",
    "\n",
    "    # Step 6: Evaluate the model\n",
    "    evaluate_model_task = evaluate_model(\n",
    "        val_x=split_data_task.outputs['x_val_df'],\n",
    "        val_y=split_data_task.outputs['y_val_df'],\n",
    "        trained_model=train_model_task.outputs['trained_model'],\n",
    "        fitted_scaler=fit_scaler_task.outputs['fitted_scaler']\n",
    "    )\n",
    "\n",
    "    # Step 7: Test the model and visualize\n",
    "    test_model_task = test_model(\n",
    "        test_x=read_data_task.outputs['test_df'],\n",
    "        trained_model=train_model_task.outputs['trained_model'],\n",
    "        fitted_scaler=fit_scaler_task.outputs['fitted_scaler'],\n",
    "        column_x=scatter_plot_column_x,\n",
    "        column_y=scatter_plot_column_y\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b8c2c-93e5-450d-94f4-4c87be3535fc",
   "metadata": {},
   "source": [
    "## Compile and run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbb73e-d0c3-400d-aa6f-4c88feeba48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Kubeflow Pipelines client\n",
    "client = Client()\n",
    "\n",
    "# Define the arguments to be passed to the pipeline\n",
    "args = dict(\n",
    "    minio_train_data_path='minio://kubeflow-examples/mobile-price-classification/train.csv',\n",
    "    minio_test_data_path='minio://kubeflow-examples/mobile-price-classification/test.csv',\n",
    "    test_size=0.2,\n",
    "    C=[1, 0.1, 0.25, 0.5, 2, 0.75],\n",
    "    kernel=[\"linear\", \"rbf\"],\n",
    "    gamma=[\"auto\", 0.01, 0.001, 0.0001, 1],\n",
    "    decision_function_shape=[\"ovo\", \"ovr\"],\n",
    "    scatter_plot_column_x='ram',\n",
    "    scatter_plot_column_y='battery_power',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create a new run from the pipeline function\n",
    "client.create_run_from_pipeline_func(\n",
    "    mobile_price_classification_pipeline,\n",
    "    arguments=args,\n",
    "    experiment_name=\"mobile-price-classification\",\n",
    "    enable_caching=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73f8fc",
   "metadata": {},
   "source": [
    "# Debugging with Lightweight Components\n",
    "Debugging can be challenging when using lightweight components in Kubeflow Pipelines. A practical approach is to download the artifacts from the steps preceding the failing one, from MinIO, and then run the functions used in the components locally. You can easily copy the paths to these artifacts from the Kubeflow Pipelines UI. Once you have these paths, you can use them as shown in the example below to download and read in Pandas DataFrames or perform similar operations. Make sure to adjust the paths according to your specific setup requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f6c39-4341-45a8-8c24-a81e0a92225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options={\n",
    "        \"key\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        \"secret\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        \"client_kwargs\": {\"endpoint_url\": f'http://{os.environ[\"S3_ENDPOINT\"]}'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1655a-2e79-49c7-aae3-d698169cb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading pandas dataframes (Modify paths!)\n",
    "x_train_path = 's3://mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/f053e118-abc8-43f0-a1a2-5b8a11156287/split-data/x_train_df'\n",
    "y_train_path = 's3://mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/f053e118-abc8-43f0-a1a2-5b8a11156287/split-data/y_train_df'\n",
    "x_val_path = 's3://mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/f053e118-abc8-43f0-a1a2-5b8a11156287/split-data/x_val_df'\n",
    "y_val_path = 's3://mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/f053e118-abc8-43f0-a1a2-5b8a11156287/split-data/y_val_df'\n",
    "x_test_path = 's3://mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/23e796ae-df8e-4b4b-a36c-aad4a85da4b0/read-data/test_df'\n",
    "raw_train_data_path = 's3://mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/23e796ae-df8e-4b4b-a36c-aad4a85da4b0/read-data/train_df'\n",
    "x_train_raw, x_train, y_train, x_val, y_val, x_test = [\n",
    "    pd.read_parquet(path, storage_options=storage_options) for path in (raw_train_data_path, x_train_path, y_train_path, x_val_path, y_val_path, x_test_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc60c0-714e-4623-bc2c-b3366de6f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the mc tool to download model artifact\n",
    "!mc cp minio/mlpipeline/v2/artifacts/mmobile-price-classification-pipeline/90e172f1-5143-475d-b02c-92fbc34338cb/train-model/trained_model ./trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a9baa-e110-45f1-90e9-c6eb2df71d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "svm_model = load('./trained_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f27993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
